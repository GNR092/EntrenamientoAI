{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eaae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38e5898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a897ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- PARÁMETROS DEL DATASET -----------------\n",
    "# Define la ruta a tu carpeta con las imágenes\n",
    "data_dir = 'assets_num/' \n",
    "# Define el tamaño al que quieres redimensionar las imágenes\n",
    "img_height = 28\n",
    "img_width = 28\n",
    "batch_size = 32\n",
    "\n",
    "# Carga los datos desde el directorio sin redimensionar de forma automática\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width), # <-- Pasa el tamaño aquí\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width), # <-- Pasa el tamaño aquí\n",
    "    batch_size=batch_size,\n",
    "    color_mode='grayscale'\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Clases encontradas:\", class_names)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Normalización: ahora solo necesitamos escalar los valores\n",
    "def normalize_image(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "# Aplica la normalización a ambos conjuntos de datos\n",
    "train_dataset = train_dataset.map(normalize_image)\n",
    "test_dataset = test_dataset.map(normalize_image)\n",
    "\n",
    "# Cache y prefetch para mejorar el rendimiento\n",
    "train_dataset = train_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cfa9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_labels, images):\n",
    "    predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img[...,0], cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"Prediccion: {}\".format(class_names[predicted_label]), color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array[i], true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(num_classes), predictions_array, color=\"#888888\")\n",
    "    plt.ylim([0,1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c03dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(img_height, img_width, 1)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.BatchNormalization(), # <-- Normalización por lotes\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.BatchNormalization(), # <-- Normalización por lotes\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5), # <-- Dropout (ej. 50%)\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "    # ----------------- COMPILACIÓN Y ENTRENAMIENTO -----------------\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd438c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "log_dir = \"logs/num/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01560b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "        train_dataset, \n",
    "        epochs=100, \n",
    "        validation_data=test_dataset,\n",
    "        callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(\"Resultado en las pruebas: \", test_accuracy)\n",
    "for test_images, test_labels in test_dataset.take(1):\n",
    "    test_images = test_images.numpy()\n",
    "    test_labels = test_labels.numpy()\n",
    "    predictions = model.predict(test_images)\n",
    "    \n",
    "model.save('numeros2.keras') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c527e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'logs/num/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef43b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!del /q \"C:\\Users\\shado\\AppData\\Local\\Temp\\.tensorboard-info\\*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352510e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b70d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook.display(port=6006, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5fc7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ca2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir 'logs'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
